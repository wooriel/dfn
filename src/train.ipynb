{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "extended-uzbekistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Dataset\n",
    "\n",
    "# from Load import load_shot, load_eig\n",
    "from FAUSTDataset import FAUSTDataset\n",
    "from ResNet import DFM\n",
    "from EuclideanLoss import EuclideanLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "stainless-rochester",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/'\n",
    "train_reg_shot = 'training/reg_shot/'\n",
    "train_scan_shot = 'training/scan_shot/'\n",
    "train_eigen = 'training/reg_lb/'\n",
    "train_dist = 'training/l2_dist/'\n",
    "test_scan_shot = 'test/scan_shot/'\n",
    "test_eigen = 'test/scan_lb/'\n",
    "inter = 'inter_challenge.txt'\n",
    "intra = 'intra_challenge.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "available-february",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr=True\n",
    "batch_size=4\n",
    "lr = 0.001\n",
    "max_epoch=50\n",
    "eignum = 120\n",
    "num_layer=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "criminal-technical",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/50] Loss: 1454.5574951171875\n",
      "[1/50] Loss: 1559.9486083984375\n",
      "[2/50] Loss: 1458.0335693359375\n",
      "[3/50] Loss: 1520.697021484375\n",
      "[4/50] Loss: 1632.489990234375\n",
      "[5/50] Loss: 1457.59033203125\n",
      "[6/50] Loss: 1432.615966796875\n",
      "[7/50] Loss: 1541.205078125\n",
      "[8/50] Loss: 1454.70166015625\n",
      "[9/50] Loss: 1522.05126953125\n",
      "[10/50] Loss: 1449.7593994140625\n",
      "[11/50] Loss: 1447.366455078125\n",
      "[12/50] Loss: 1503.822998046875\n",
      "[13/50] Loss: 1398.0615234375\n",
      "[14/50] Loss: 1517.6575927734375\n",
      "[15/50] Loss: 1441.2149658203125\n",
      "[16/50] Loss: 1398.0615234375\n",
      "[17/50] Loss: 1471.8585205078125\n",
      "[18/50] Loss: 1581.234375\n",
      "[19/50] Loss: 1440.0350341796875\n",
      "[20/50] Loss: 1460.638671875\n",
      "[21/50] Loss: 1467.613525390625\n",
      "[22/50] Loss: 1503.2357177734375\n",
      "[23/50] Loss: 1447.366455078125\n",
      "[24/50] Loss: 1573.1463623046875\n",
      "[25/50] Loss: 1510.483154296875\n",
      "[26/50] Loss: 1531.72900390625\n",
      "[27/50] Loss: 1589.8702392578125\n",
      "[28/50] Loss: 1531.72900390625\n",
      "[29/50] Loss: 1577.4384765625\n",
      "[30/50] Loss: 1474.22265625\n",
      "[31/50] Loss: 1588.0467529296875\n",
      "[32/50] Loss: 1531.318115234375\n",
      "[33/50] Loss: 1574.646484375\n",
      "[34/50] Loss: 1618.992919921875\n",
      "[35/50] Loss: 1487.1337890625\n",
      "[36/50] Loss: 1510.6585693359375\n",
      "[37/50] Loss: 1390.41015625\n",
      "[38/50] Loss: 1478.280517578125\n",
      "[39/50] Loss: 1581.4056396484375\n",
      "[40/50] Loss: 1562.7574462890625\n",
      "[41/50] Loss: 1449.75927734375\n",
      "[42/50] Loss: 1538.0325927734375\n",
      "[43/50] Loss: 1495.570068359375\n",
      "[44/50] Loss: 1572.0751953125\n",
      "[45/50] Loss: 1487.1337890625\n",
      "[46/50] Loss: 1518.96435546875\n",
      "[47/50] Loss: 1517.6575927734375\n",
      "[48/50] Loss: 1503.822998046875\n",
      "[49/50] Loss: 1516.04248046875\n"
     ]
    }
   ],
   "source": [
    "# load training data\n",
    "# shot = []\n",
    "# eigen = []\n",
    "# dist = []\n",
    "# if tr:\n",
    "#     streig = \"%d\" % (eignum)\n",
    "#     for i in range(100):\n",
    "#         strnum = '%03d' % (i)\n",
    "# #                 data_dir, train_reg_shot, \n",
    "#         fn = ''.join([data_dir, train_reg_shot, 'tr_reg_res_', strnum, '.txt'])\n",
    "#         shot.append(load_shot(fn))\n",
    "#         fn_eig = ''.join([data_dir, train_eigen, 'tr_reg_', streig, '_', strnum, '.h5'])\n",
    "#         eigen.append(load_eig(fn_eig))\n",
    "# #         fn_dist = ''.join([data_dir, train_dist, 'tr_reg_dist_', strnum, '.h5'])\n",
    "# #         dist.append(load_dist(fn_dist))\n",
    "# print('loaded all')\n",
    "    \n",
    "train_dataset = FAUSTDataset(tr, batch_size)\n",
    "\n",
    "train_loader = Dataset.DataLoader(dataset=train_dataset, batch_size=batch_size,\n",
    "                          shuffle=True, num_workers=0, drop_last=False\n",
    "                         )\n",
    "\n",
    "# import model\n",
    "model = DFM(tr, num_layer)\n",
    "model = model.float()\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), eps=1e-08)\n",
    "\n",
    "# loss\n",
    "criterion = EuclideanLoss()\n",
    "\n",
    "for epoch in range(0, max_epoch):\n",
    "    if epoch % 100 > 0:\n",
    "        lr *= 0.1\n",
    "        \n",
    "    total_loss = 0\n",
    "    count = 0\n",
    "    model.train()\n",
    "    for data in enumerate(train_loader):\n",
    "#         print(len(data[1]))\n",
    "#         print(data)\n",
    "        _, data = data # data came out as (0, [6 different data])\n",
    "        src_shot, tar_shot, src_eigen, tar_eigen, src_dist, tar_dist = data\n",
    "        src_shot = src_shot.float()\n",
    "        tar_shot = tar_shot.float()\n",
    "        src_eigen = src_eigen.float()\n",
    "        tar_eigen = tar_eigen.float()\n",
    "        src_dist = src_dist.float()\n",
    "        tar_dist = tar_dist.float()\n",
    "#         print(src_shot.shape)\n",
    "#         print(tar_shot.shape)\n",
    "#         print(src_eigen.shape)\n",
    "#         print(tar_eigen.shape)\n",
    "#         print(src_dist.shape)\n",
    "#         print(tar_dist.shape)\n",
    "#         print(type(src_shot))\n",
    "#         print(type(tar_shot))\n",
    "#         print(type(src_eigen))\n",
    "#         print(type(tar_eigen))\n",
    "#         print(type(src_dist))\n",
    "#         print(type(tar_dist))\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # P: soft corr matrix\n",
    "        P, C = model(src_shot, tar_shot, src_eigen, tar_eigen)\n",
    "        loss = criterion(P, src_dist, tar_dist)\n",
    "        total_loss += loss\n",
    "        count += 4\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(\"[{0}/{1}] Loss: {2}\".format(epoch, max_epoch, total_loss/count))\n",
    "    \n",
    "    if epoch % 10 == 9:\n",
    "        torch.save(model.state_dict(), os.path.join('{}.pth'.format(epoch+1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broad-watershed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
